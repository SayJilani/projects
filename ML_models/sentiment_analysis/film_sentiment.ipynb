{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc01f4c-9b8f-4723-8a5c-6c2d8d873b94",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "dataset link: https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis?resource=download\n",
    "\n",
    "(also in this folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f62f1d-e035-43ff-88b3-2a0a5739563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sayyedjilani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a99e55-be57-49a5-aa20-f950321a9123",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be9fb66-b0cd-4f03-930d-841c1703c4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dataset and labels\n",
    "movies_df = pd.read_csv('./../../../movie.csv')\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8f3a32f-a25c-4948-95fa-797247b8a83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    20019\n",
       "1    19981\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view distribution of labels\n",
    "movies_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09c5539d-d232-4fcb-93b3-afc0ff73c535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view example text\n",
    "movies_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e985f-304f-4a8e-a7c3-04559f5ab964",
   "metadata": {},
   "source": [
    "The text iteslf needs to be processed, which includes removing stop words and break each word down to its indivdual stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b714f99e-941a-4199-a51b-e15ccc87ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest positive review: 13704\n",
      "Longest negative review: 8969\n",
      "\n",
      "Longest positive review: 1328.0832791151595\n",
      "Longest negative review: 1292.5369898596334\n"
     ]
    }
   ],
   "source": [
    "X = movies_df.drop('label', axis=1).values\n",
    "y = movies_df['label'].values\n",
    "\n",
    "# a list of lengths for training examples with a positive label.\n",
    "text_lengths_pos = [len(x) for (i, x) in enumerate(np.squeeze(X)) if y[i]]\n",
    "\n",
    "# a list of lengths for training examples with a negative label.\n",
    "text_lengths_neg = [len(x) for (i, x) in enumerate(np.squeeze(X)) if not y[i]]\n",
    "\n",
    "\n",
    "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 10000), label=['positive', 'negative'])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# check the longest reviews\n",
    "print('Longest positive review:', max(text_lengths_pos))\n",
    "print('Longest negative review:', max(text_lengths_neg))\n",
    "# check avg review length\n",
    "print('\\nLongest positive review:', np.mean(text_lengths_pos))\n",
    "print('Longest negative review:', np.mean(text_lengths_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6916d66-d98f-414f-ab17-87493d04b557",
   "metadata": {},
   "source": [
    "On average the lengths of the reviews are quite similar. Similarly, the distribution of word lengths is also quite similar across the two classes, and the very large lengths can be considered to be outliers. Given the similar distributions, applying a word count limit as part of our processing may not be neccessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e895f0ae-5f8c-4307-b72a-32cbf4ca11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset \n",
    "X = movies_df.drop('label', axis=1).values\n",
    "y = movies_df['label'].values\n",
    "\n",
    "# split the data into training and temporary set (80% training + 20% temp set)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# split the temp set into testing and validation sets (80% training, 10% validation and 10% test set of the original dataset)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7adf92a-37da-453d-8fe2-37049829c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes\n",
      "\n",
      "X_train:  (32000,)\n",
      "X_val:  (4000,)\n",
      "X_test:  (4000,)\n",
      "y_train:  (32000,)\n",
      "y_val:  (4000,)\n",
      "y_test:  (4000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.squeeze(X_train)\n",
    "X_val = np.squeeze(X_val)\n",
    "X_test = np.squeeze(X_test)\n",
    "\n",
    "print('split sizes\\n')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_val: ', y_val.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc989b5d-8cda-4591-8ad0-32ed7adab188",
   "metadata": {},
   "source": [
    "Do positive film reviews tend to have greater length, or is it the other way around? We can utilze histograms to see the distibution of review length by sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5acabc26-09ee-4d4e-8422-b253241f55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    '''\n",
    "    Input:\n",
    "        text: a string containing a film review\n",
    "    Output:\n",
    "        clean_text: a list of words containing the processed text\n",
    "    '''\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # tokenize text by only using concurrent letters\n",
    "    words = re.findall(r'[a-z]+', text.lower())\n",
    "\n",
    "    clean_text = []\n",
    "    for word in words:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            clean_text.append(stem_word)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e5878-8360-445e-9212-69c5abbce392",
   "metadata": {},
   "source": [
    "It should be noted, the processing would take careful consideration of the application. Given our goal is to infer sentiment, punctuation may not be required as sentence structure may not play a part in inferring sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4ab7a55-6060-4916-9caa-e404e46e1532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is an example text:\n",
      "I watched it last night and again this morning - that's how much I liked it. There is something about this movie... When the movie was almost over, I was about to cry. I would strongly recommend \"Latter Days\" to my friends - it's definitely worth seeing! I agree with those who say that some parts of the movie do not look very realistic. For example, both main characters are totally cute and in perfect physical shape (although, round is also a type of shape:) ). I rarely meet people like this as singles and I have never met any in couples. Other parts of the movie, including all those \"coincidences\", do not look very realistic as well. BUT, after all it is A MOVIE, not a life story.\n",
      "\n",
      "Thefollowing is an example of processed text:\n",
      "['watch', 'last', 'night', 'morn', 'much', 'like', 'someth', 'movi', 'movi', 'almost', 'cri', 'would', 'strongli', 'recommend', 'latter', 'day', 'friend', 'definit', 'worth', 'see', 'agre', 'say', 'part', 'movi', 'look', 'realist', 'exampl', 'main', 'charact', 'total', 'cute', 'perfect', 'physic', 'shape', 'although', 'round', 'also', 'type', 'shape', 'rare', 'meet', 'peopl', 'like', 'singl', 'never', 'met', 'coupl', 'part', 'movi', 'includ', 'coincid', 'look', 'realist', 'well', 'movi', 'life', 'stori']\n"
     ]
    }
   ],
   "source": [
    "print('The following is an example text:')\n",
    "print(X_train[0])\n",
    "print('\\nThefollowing is an example of processed text:')\n",
    "print(process_text(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fd496c5-9655-4c54-8671-c7b722a5825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with extremely large datasets (GB to TB), the counts should not be stored in a dictionary. Splitting the data and utilzing the MapReduce paradigm\n",
    "# over various nodes may become necessary \n",
    "\n",
    "def count_text(mapper, X, y):\n",
    "    '''\n",
    "    Input:\n",
    "        mapper: dcitionary used to store tuples of word and sentiment counts\n",
    "        X: an array of texts\n",
    "        y: an array of corresponding sentiment of each text \n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency { (word, sentiment) : count }\n",
    "    '''\n",
    "    \n",
    "    for y, text in zip(y, X):\n",
    "        for word in process_text(text):\n",
    "\n",
    "            # word, sentiment tuple as kay\n",
    "            key = (word, y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if mapper.get(key,0) != 0:\n",
    "                mapper[key] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                mapper[key] = 1\n",
    "\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddafe9a0-a6d9-423e-8e1e-56ce052319d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('watch', 1): 1,\n",
       " ('last', 1): 1,\n",
       " ('night', 1): 1,\n",
       " ('morn', 1): 1,\n",
       " ('much', 1): 1,\n",
       " ('like', 1): 3,\n",
       " ('someth', 1): 4,\n",
       " ('movi', 1): 5,\n",
       " ('almost', 1): 1,\n",
       " ('cri', 1): 1,\n",
       " ('would', 1): 1,\n",
       " ('strongli', 1): 1,\n",
       " ('recommend', 1): 1,\n",
       " ('latter', 1): 1,\n",
       " ('day', 1): 1,\n",
       " ('friend', 1): 1,\n",
       " ('definit', 1): 1,\n",
       " ('worth', 1): 1,\n",
       " ('see', 1): 1,\n",
       " ('agre', 1): 1,\n",
       " ('say', 1): 2,\n",
       " ('part', 1): 2,\n",
       " ('look', 1): 2,\n",
       " ('realist', 1): 2,\n",
       " ('exampl', 1): 1,\n",
       " ('main', 1): 1,\n",
       " ('charact', 1): 2,\n",
       " ('total', 1): 1,\n",
       " ('cute', 1): 1,\n",
       " ('perfect', 1): 1,\n",
       " ('physic', 1): 1,\n",
       " ('shape', 1): 2,\n",
       " ('although', 1): 2,\n",
       " ('round', 1): 1,\n",
       " ('also', 1): 1,\n",
       " ('type', 1): 1,\n",
       " ('rare', 1): 1,\n",
       " ('meet', 1): 1,\n",
       " ('peopl', 1): 1,\n",
       " ('singl', 1): 1,\n",
       " ('never', 1): 1,\n",
       " ('met', 1): 1,\n",
       " ('coupl', 1): 1,\n",
       " ('includ', 1): 1,\n",
       " ('coincid', 1): 1,\n",
       " ('well', 1): 1,\n",
       " ('life', 1): 1,\n",
       " ('stori', 1): 2,\n",
       " ('western', 1): 2,\n",
       " ('one', 1): 2,\n",
       " ('favourit', 1): 1,\n",
       " ('john', 1): 2,\n",
       " ('ford', 1): 3,\n",
       " ('lack', 1): 1,\n",
       " ('certain', 1): 1,\n",
       " ('film', 1): 1,\n",
       " ('seen', 1): 1,\n",
       " ('anyway', 1): 2,\n",
       " ('possess', 1): 1,\n",
       " ('nit', 1): 1,\n",
       " ('sure', 1): 1,\n",
       " ('tangibl', 1): 1,\n",
       " ('gist', 1): 1,\n",
       " ('mormon', 1): 1,\n",
       " ('wagon', 1): 2,\n",
       " ('train', 1): 1,\n",
       " ('use', 1): 1,\n",
       " ('band', 1): 1,\n",
       " ('outlaw', 1): 1,\n",
       " ('hideout', 1): 1,\n",
       " ('pursu', 1): 1,\n",
       " ('poss', 1): 1,\n",
       " ('employ', 1): 1,\n",
       " ('lot', 1): 1,\n",
       " ('regular', 1): 1,\n",
       " ('interest', 1): 1,\n",
       " ('nice', 1): 1,\n",
       " ('sceneri', 1): 1,\n",
       " ('bit', 1): 1,\n",
       " ('action', 1): 1,\n",
       " ('excit', 1): 1,\n",
       " ('add', 1): 1,\n",
       " ('watchabl', 1): 1,\n",
       " ('experi', 1): 1,\n",
       " ('certainli', 1): 1,\n",
       " ('bore', 1): 1,\n",
       " ('quit', 1): 1,\n",
       " ('usual', 1): 1,\n",
       " ('standard', 1): 1,\n",
       " ('master', 1): 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing functionality of count_text\n",
    "count_text({}, X_train[0:2], y_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f01e495-7e80-4aa7-8023-72f24d2b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training frequencies \n",
    "train_freq = count_text({}, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed1c90-8611-48ff-b72f-d40b3a1f359f",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00b2a1-2816-438c-90d5-97b8838c2f46",
   "metadata": {},
   "source": [
    "#### Approach 1: Logistic Regression \n",
    "\n",
    "To train this, we will be hand-coding the logistic regression (instead of letting other packages do the work for us). The goal here will be to iteate until we find the correct weights that will minimize the cost function. We will be utilizing the following functions:\n",
    "\n",
    "\n",
    "##### Sigmoid function\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "As the next step, we will be using the cost function used specifically for logistic regression. This is the average of the log loss across all training examples:\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "* $\\theta$ represents the weights associated with the features.\n",
    "\n",
    "Each indidual training data will have a bias term, positive term and negative term associated with the word\n",
    "\n",
    "#### Gradient Descent\n",
    "We will then proceed to using gradient descent for the purposes of our training. \n",
    "\n",
    "* training will be done for a set number of iterations\n",
    "* for each iteration, we will calulate the cost from the cost function by utilizing all of the training data \n",
    "* we will then update the learned weights associated with our function using a set `alpha`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b39355ec-6731-46b8-b998-718c221a0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting functions for training \n",
    "\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    h = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters, decay_factor=0.9):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is of dimension (m,3), where m is the number of training samples\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: initialized weight vector of dimension (3,1) with zeros\n",
    "        alpha: learning rate (initialized at 0.05)\n",
    "        num_iters: number of iterations to train the model for\n",
    "        decay_factor: apply a decay to alpha after each iteration to start at large alpha, and get slower with iterations\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: the final weight vector\n",
    "    '''\n",
    "\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        # get z, the dot product of x and theta\n",
    "        z = x @ theta\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -1/m * np.sum(y.T @ np.log(h+1e-20) + (1-y).T @ np.log(1-h+1e-20))\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - alpha/m * np.sum(x.T @ (h-y), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"running iteration {i}\")\n",
    "            print(f\"current loss {J}\")\n",
    "            print(f\"current alpha {alpha:.12f}\")\n",
    "            print()\n",
    "\n",
    "        # apply decay factor after each iteration\n",
    "        alpha *= decay_factor\n",
    "\n",
    "    return J, theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(text, freqs, process_text=process_text):\n",
    "    '''\n",
    "    Input: \n",
    "        text: string containing a film review\n",
    "        freqs: a dictionary corresponding to the count of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_text tokenizes, stems, and removes stopwords\n",
    "    word_list = process_text(text)\n",
    "    \n",
    "    # 3 elements for [bias, positive, negative] counts\n",
    "    x = np.zeros(3) \n",
    "    \n",
    "    # bias term is set to 1\n",
    "    x[0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_list:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[1] += freqs.get((word, 1), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[2] += freqs.get((word, 0), 0)\n",
    "        \n",
    "    \n",
    "    x = x[None, :]  \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14f4c211-1deb-43ae-a8f8-704903218264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model to retrieve final weights\n",
    "X = np.zeros((len(X_train), 3))\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X[i, :]= extract_features(X_train[i], train_freq)\n",
    "\n",
    "Y = y_train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40243f0c-ee82-4793-beac-59167123e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 0\n",
      "current loss 0.6931471805600389\n",
      "current alpha 0.500000000000\n",
      "\n",
      "running iteration 50\n",
      "current loss 22.949577798733902\n",
      "current alpha 0.002576887604\n",
      "\n",
      "running iteration 100\n",
      "current loss 23.102124061144693\n",
      "current alpha 0.000013280699\n",
      "\n",
      "running iteration 150\n",
      "current loss 22.949577798733902\n",
      "current alpha 0.000000068446\n",
      "\n",
      "running iteration 200\n",
      "current loss 22.949577798733902\n",
      "current alpha 0.000000000353\n",
      "\n",
      "running iteration 250\n",
      "current loss 22.949577798733902\n",
      "current alpha 0.000000000002\n",
      "\n",
      "running iteration 300\n",
      "current loss 21.00945452995878\n",
      "current alpha 0.000000000000\n",
      "\n",
      "running iteration 350\n",
      "current loss 0.6954113652107456\n",
      "current alpha 0.000000000000\n",
      "\n",
      "running iteration 400\n",
      "current loss 0.6954054348749223\n",
      "current alpha 0.000000000000\n",
      "\n",
      "running iteration 450\n",
      "current loss 0.6954054043533008\n",
      "current alpha 0.000000000000\n",
      "\n",
      "The cost after training is 0.6954054041960899.\n",
      "The resulting vector of weights is [[-4.62379890e-11]\n",
      " [-3.10528070e-07]\n",
      " [ 2.89652789e-07]]\n"
     ]
    }
   ],
   "source": [
    "# initializing theta with zeroes, alpha at 0.5, running for 1000 epochs\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 0.5, 500) \n",
    "print(f\"The cost after training is {J}.\")\n",
    "print(f\"The resulting vector of weights is {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4065d81-9706-471a-bea1-724e40ab6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_review(film_review, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        film_review: a film review\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights trained\n",
    "    Output: \n",
    "        y_pred: the probability of a review being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the film review \n",
    "    x = extract_features(film_review, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(x @ theta)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "853b866a-66af-422f-ae0b-23988061ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.49375\n"
     ]
    }
   ],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_review=predict_logistic_review):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of reviews\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of reviews\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of reviews classified correctly) / (total # of reviews)\n",
    "    \"\"\"\n",
    "        \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for review in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_review(review, freqs, theta)\n",
    "\n",
    "        # using 0.7 as our threshold \n",
    "        if y_pred > 0.7:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = np.ravel(test_y)\n",
    "    \n",
    "    accuracy = np.sum(y_hat == test_y)/len(test_y)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# test accuracy of validation dataset using our trained theta\n",
    "accuracy = test_logistic_regression(X_val, y_val, train_freq, theta)\n",
    "print(f\"Logistic regression model's accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9767a-71af-4ffa-9f5d-3da7de89c542",
   "metadata": {},
   "source": [
    "Our logistic regression approach is less than ideal. This definitely makes sense, as our methodology of feature extraction does not hold any semantic meaning which would be extremely vital in inferring review sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da629e17-a04d-4fbe-a789-94d2fd95f7ef",
   "metadata": {},
   "source": [
    "#### Approach 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b51fb-f5c5-4c0d-8de9-424a4a62ca46",
   "metadata": {},
   "source": [
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "for all words in a review:\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "* if p > 0 then class is 1\n",
    "* else class is 0\n",
    "\n",
    "\n",
    "##### Probability of word given the class with Laplacien smoothing:\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$\n",
    "\n",
    "\n",
    "* Based on this equation for each word, the log likelihood will be calculated for each word\n",
    "* Based on our entire dataset, the log prior will be calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20a19596-9016-4a6c-8e1d-ecbce2c471e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of film reviews\n",
    "        train_y: a list of labels corresponding to the reviews (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. \n",
    "        loglikelihood: the log likelihood of the Naive bayes equation. \n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    vocab = set([key[0] for key in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = list(train_y).count(1)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = list(train_y).count(0)\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52d3df98-890d-4e9e-a8f4-e9c1a291379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(text, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        text: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    # process the text to get a list of words\n",
    "    word_l = process_text(text)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of reviews\n",
    "        test_y: the corresponding labels for the list of reviews\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of reviews classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  \n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.sum(np.absolute(y_hats - test_y)) / len(test_y)\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4834654a-b415-40de-9a3a-086fec089a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 85.5%\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(train_freq, X_train, y_train)\n",
    "\n",
    "print(f\"Naive Bayes accuracy = {test_naive_bayes(X_val, y_val, logprior, loglikelihood)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f0a4684b-b922-4020-803f-b4a6c14df5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare how the two models predict a pre-written review for a film\n",
    "\n",
    "# 1) complex positive review, 2) simple positive review, 3) semi-complex positive review\n",
    "reviews = [\"\"\"This film was actually not bad! I had super low expectations but this ended up exceeding them by quite a lot. The cinematography was\n",
    "quite exceptional. Although it should be noted the acting left much to be desired. Overall I had a decent time and would reccomend it.\"\"\",\n",
    "         \"\"\"I absolutely loved this film.\"\"\", \"\"\"Started out not on the best footing, but kept getting super intersting with each passing minute\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b7fcad35-f931-473e-835f-cf1fc28576c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.105376535659577\n",
      "0.5314140243777077\n",
      "-0.6470141725664993\n"
     ]
    }
   ],
   "source": [
    "# prediction from bayes approach on our text (greater than 0 is positive)\n",
    "for review in reviews:\n",
    "    print(naive_bayes_predict(review, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23a31413-2508-45c4-9d33-cd9bc829285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50013181]]\n",
      "[[0.4992102]]\n",
      "[[0.5000156]]\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    print(predict_logistic_review(review, train_freq, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd33180-4ff9-48b2-b33d-1b6fd9bd31d9",
   "metadata": {},
   "source": [
    "Logistic regression's performance as established is left to a coin-toss based on the selected features. Naive bayes on the other hand lacks any proficiency with complex reviews. We can infer that the our features do not capture any semantic meaning, and only rely on the frequencies of words. This is less than ideal, as such utilizing word emebeddings should be the next approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe881c5-737e-4281-b3bf-68f4a9637d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
